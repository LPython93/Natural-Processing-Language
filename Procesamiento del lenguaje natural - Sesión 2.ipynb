{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento del lenguaje natural\n",
    "## Word counts con bag of words\n",
    "\n",
    "Bag-of-words:\n",
    "* Método básico para encontrar 'topics' en un texto\n",
    "* Se necesita primero crear tokens utilizando tokenización\n",
    "* Habrá una una palabra frecuente, que podría ser la más importante sin contar las palabras usuales como preposiciones etc...\n",
    "* Puede ser una buena manera de determinar las palabras significativas en un texto\n",
    "\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "    `The cat is in the box. The cat likes the box. The box is over the cat`\n",
    "    \n",
    "Bag of words:\n",
    "\n",
    "* 'The','box','cat','the' : 3\n",
    "* 'is' : 2\n",
    "* 'in','likes','over':1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'cat', 'is', 'in', 'the', 'box', '.', 'The', 'box', 'is', 'over', 'the', 'cat', '.', 'The', 'box', 'is', 'over', 'the', 'cat']\n",
      "====================\n",
      "Contador de palabras\n",
      "Counter({'The': 3, 'cat': 3, 'is': 3, 'the': 3, 'box': 3, '.': 2, 'over': 2, 'in': 1})\n",
      "====================\n",
      "Las palabras más frecuentes\n",
      "[('The', 3), ('cat', 3), ('is', 3), ('the', 3)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "sentence = 'The cat is in the box. The box is over the cat. The box is over the cat'\n",
    "\n",
    "# Puesto que nos da igual que las frases estén tokenizadas, ya que buscamos palabras, nos saltamos el \n",
    "# paso de tokenizar las frases\n",
    "\n",
    "# Tokenizamos la sentence\n",
    "word_token = word_tokenize(sentence)\n",
    "print(word_token)\n",
    "\n",
    "print('====================')\n",
    "print('Contador de palabras')\n",
    "counter = Counter(word_token)\n",
    "print(counter)\n",
    "\n",
    "print('====================')\n",
    "# las más frecuentes\n",
    "print('Las palabras más frecuentes')\n",
    "print(counter.most_common(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto\n",
    "\n",
    "¿Por qué preprocesar el texto?\n",
    "* Ayuda a un mejor input data\n",
    "        - Por ejemplo, cuando hacemos machine learning or statistical methods\n",
    "* Lemmatizacion/Stemming\n",
    "    - Llevar las palabras a su raíz\n",
    "* Eliminar las llamadas 'stop words', signos de puntuación o tokens no deseados.\n",
    "\n",
    "**notas**: \n",
    "1. Paso importante, todas las palabras las transformaremos en minúsculas para evitar confusiones.\n",
    "2. Cada idioma tiene sus propias stop words (están en las librerías de Python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS en bruto\n",
      "['the', 'cat', 'is', 'in', 'the', 'box', 'the', 'cat', 'liked', 'the', 'box', 'the', 'box', 'is', 'over', 'the', 'cat']\n",
      "==========================================================\n",
      "PALABRAS NO STOP WORDS\n",
      "['cat', 'box', 'cat', 'liked', 'box', 'box', 'cat']\n",
      "==========================================================\n",
      "Palabras más frecuentes en el texto\n",
      "[('cat', 3), ('box', 3)]\n"
     ]
    }
   ],
   "source": [
    "### Ejemplo de eliminación de 'stop words'\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"\"\"The cat is in the box. The cat liked the box. The box is over the cat\"\"\"\n",
    "\n",
    "# Paso 1: Transformamos las letras en minúsculas \n",
    "# Paso 2: Tokenizamos la frase como un todo (saltándonos el paso previo de tokenizar frases)\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
    "print('TOKENS en bruto')\n",
    "print(tokens)\n",
    "\n",
    "print('==========================================================')\n",
    "\n",
    "# Paso 3: Introducimos en una lista aquellas palabras que no sean consideradas stop words\n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "print('PALABRAS NO STOP WORDS')\n",
    "print(no_stops)\n",
    "print('==========================================================')\n",
    "\n",
    "# Paso 4: contar las palabras más frecuentes del texto\n",
    "print('Palabras más frecuentes en el texto')\n",
    "print(Counter(no_stops).most_common(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 3), ('box', 3), ('liked', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos de Lematización\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Utilizando el mismo ejemplo anterior, ya tenemos nuestras palabras en minúsculas y además hemos filtrado las stopwords\n",
    "\n",
    "# Paso 5: Iniciamos el lematizador\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Paso 6: Lematizamos e introducimos en una lista\n",
    "lemmatized = [lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Paso 7: Creamos una lista de bag-of-words (palabra: veces que aparece)\n",
    "bag = Counter(lemmatized)\n",
    "\n",
    "# Paso 8: Las 10 más comunes\n",
    "print(bag.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim\n",
    "\n",
    "* Open-source de la librería de NLP\n",
    "* Utilizado para construir 'vectores' de documentos o de palabras (espacio vectorial)\n",
    "* Comparar y identificar topics en un documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lidia\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'about': 2, 'aliens': 3, 'and': 4, 'movie': 5, 'spaceship': 6, 'the': 7, 'was': 8, '!': 9, 'i': 10, 'liked': 11, 'really': 12, ',': 13, 'action': 14, 'awesome': 15, 'boring': 16, 'but': 17, 'characters': 18, 'scenes': 19, 'alien': 20, 'awful': 21, 'films': 22, 'hate': 23, 'cool': 24, 'is': 25, 'space': 26, 'more': 27, 'please': 28}\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)], [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)], [(0, 1), (5, 1), (7, 1), (8, 1), (9, 1), (10, 1), (20, 1), (21, 1), (22, 1), (23, 1)], [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)], [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo\n",
    "\n",
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "'I really liked the movie!',\n",
    "'Awesome action scenes, but boring characters.',\n",
    "'The movie was awful! I hate alien films.',\n",
    "'Space is cool! I liked the movie.',\n",
    "'More space films, please!']\n",
    "\n",
    "# Paso 1: Transformamos las letras en minúsculas\n",
    "doc_lower = []\n",
    "for i in my_documents:\n",
    "    x = i.lower()\n",
    "    doc_lower.append(x)\n",
    "\n",
    "# Paso 2: Tokenizamos cada frase con word_tokenize\n",
    "tokenized_words = []\n",
    "for i in doc_lower:\n",
    "    x = word_tokenize(i)\n",
    "    tokenized_words.append(x)\n",
    "    \n",
    "# Paso 3: Creamos un diccionario de palabras (asociando palabra:n veces)\n",
    "dictionary = Dictionary(tokenized_words)\n",
    "dictionary.token2id\n",
    "print(dictionary.token2id)\n",
    "\n",
    "# Paso 4: Transformamos cada palabra en un número y le asociamos la frecuencia\n",
    "corpus = []\n",
    "for i in tokenized_words:\n",
    "    x = dictionary.doc2bow(i)\n",
    "    corpus.append(x)\n",
    "print(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El corpus que hemos creado es una lista de listas. Cada lista representa un documento.\n",
    "* Los espacios vectoriales que se asocian con la frecuencia y repetición de palabras son muy útiles para procesos de deep learning. Nos permiten observar también relaciones entre palabras dentro del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id de la palabra computer\n",
      "3\n",
      "===============================================\n",
      "computer\n",
      "===============================================\n",
      "[[(0, 2), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 3), (20, 2), (21, 1), (22, 1), (23, 1), (24, 2), (25, 1)]]\n",
      "=====================================================\n",
      "[(0, 2), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "# ejemplo ya tokenizado\n",
    "articles = [['uses',\n",
    "  'file',\n",
    "  'operating',\n",
    "  'system',\n",
    "  'placement',\n",
    "  'software',\n",
    "  'diagram', 'showing',\n",
    "  'user','computing','user','interacts','application', 'software','typical','desktop','computer','the',\n",
    "  'application','software',  'layer','interfaces','operating',\n",
    "  'system','turn','communicates','personal','computer','hardware','arrows',\n",
    "  'indicate','information','flow']]\n",
    "\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# id asociado de la palabra 'computer'\n",
    "computer_id = dictionary.token2id.get('computer')\n",
    "print('id de la palabra computer')\n",
    "print(computer_id)\n",
    "\n",
    "print('===============================================')\n",
    "# a partir del id, sacar la palabra\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "print('===============================================')\n",
    "corpus = []\n",
    "for article in articles:\n",
    "    x = dictionary.doc2bow(article)\n",
    "    corpus.append(x)\n",
    "print(corpus)\n",
    "\n",
    "print('=====================================================')\n",
    "print(corpus[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software 3\n",
      "application 2\n",
      "computer 2\n",
      "operating 2\n",
      "system 2\n"
     ]
    }
   ],
   "source": [
    "# Guardar un documento\n",
    "doc = corpus[0]\n",
    "\n",
    "# palabra- frecuencia (invertimos el orden)\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse = True)\n",
    "\n",
    "# Las 5 primeras palabras que más relevancia tienen (mayor frecuencia)\n",
    "for word_id,word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id),word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tf - idf gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Permite determinar las palabras más importantes en un documento.\n",
    "* Idea: las palabras más comunes en un documento son como las stopwords y deben eliminarse o al menos bajarle el grado de importancia.\n",
    "* Ejemplo: Si yo soy un astrónomo, la palabra 'sky' deberá usarse de manera frecuente pero no es importante por lo que le bajo el grado de importancia.\n",
    "* Asegura que las palabras más comúnes no aparezcan como 'key words' del texto.\n",
    "* A cada palabra se le asocia un peso (grado de importancia).\n",
    "\n",
    "* formula $w_{i,j} = tf_{i,j}\\log\\frac{N}{df_{i}}$\n",
    "* $w_{i,j} = $ tf-idf weight for token i in document j\n",
    "* $tf_{i} = $ number of occurences of token i in document j\n",
    "* $df_{i} = $ number of documents that contain token i \n",
    "* $N$ = total number of documents\n",
    "\n",
    "nota: cuanto menos aparezca, mas grande será el logaritmo y por tanto, mayor será el peso.\n",
    "\n",
    "Para utilizar esta librería ya hemos transformado todo el texto a un ejemplo anterior del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "corpus = [[(0, 2),\n",
    "  (1, 1),\n",
    "  (2, 4),\n",
    "  (3, 1),\n",
    "  (4, 2),\n",
    "  (5, 2),\n",
    "  (6, 1),\n",
    "  (7, 1)],\n",
    "  [(8, 5),\n",
    "  (9, 2),\n",
    "  (10, 2),\n",
    "  (11, 1),\n",
    "  (12, 1),\n",
    "  (13, 1),\n",
    "  (14, 2),\n",
    "  (15, 1),\n",
    "  (16, 1),\n",
    "  (17, 2),\n",
    "  (18, 1),\n",
    "  (19, 3),\n",
    "  (20, 1),\n",
    "  (21, 4),\n",
    "  (22, 1),\n",
    "  (23, 3),\n",
    "  (24, 2),\n",
    "  (25, 1),\n",
    "  (26, 1),\n",
    "  (27, 4)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.2857142857142857), (1, 0.14285714285714285), (2, 0.14285714285714285), (3, 0.2857142857142857), (4, 0.14285714285714285), (5, 0.14285714285714285), (6, 0.14285714285714285), (7, 0.14285714285714285), (8, 0.14285714285714285), (9, 0.14285714285714285), (10, 0.14285714285714285), (11, 0.14285714285714285), (12, 0.14285714285714285), (13, 0.14285714285714285), (14, 0.14285714285714285), (15, 0.2857142857142857), (16, 0.14285714285714285), (17, 0.14285714285714285), (18, 0.14285714285714285), (19, 0.42857142857142855), (20, 0.2857142857142857), (21, 0.14285714285714285), (22, 0.14285714285714285), (23, 0.14285714285714285), (24, 0.2857142857142857), (25, 0.14285714285714285)]\n"
     ]
    }
   ],
   "source": [
    "# Creamos a new TfidfModel uando el corpus\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculamos the tfidf weights del doc:\n",
    "tfidf_weights = tfidf[doc]\n",
    "print(tfidf_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "software 0.42857142857142855\n",
      "application 0.2857142857142857\n",
      "computer 0.2857142857142857\n",
      "operating 0.2857142857142857\n",
      "system 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "# ordenamos los pesos , haciendo también un cambio de posición\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Mostramos las palabras con más relevancia:\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
