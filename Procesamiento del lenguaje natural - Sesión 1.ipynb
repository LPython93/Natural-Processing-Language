{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento del lenguaje natural\n",
    "\n",
    "### Expresiones regulares\n",
    "\n",
    "* Expresiones con una sintaxis especial\n",
    "* Podemos trabajar con ellas debido a que tienen un patrón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libería de expresiones regulares\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python `re` modulos:\n",
    "* `re`module\n",
    "* `split`: split a string on regex\n",
    "* `findall`: find all patterns in a string\n",
    "* `search`: search for a pattern\n",
    "* `match`: match an entire string or substring based on a pattern\n",
    "\n",
    "En los comandos primero el patrón y luego la frase\n",
    "\n",
    "#### Patrones comunes:\n",
    "\n",
    "    pattern   matches    example\n",
    "       \\w+       word      'Magic'\n",
    "       \\d        digit        9\n",
    "       \\s        space       ''\n",
    "       .*       wildcare    'username29'\n",
    "      + or     greedy match 'aaaaaa'\n",
    "      \\S        not space   'not_space'\n",
    "      [a-z]  lowercase group 'abdcefg'\n",
    "\n",
    "**nota**: wildcare nos permite hacer un 'match' con cualquier símbolo o letra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "# re.match 'matchea entre dos expresiones\n",
    "print(re.match('abc','abcdef'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ']\n",
      "['Hola', 'Mi', 'nombre', 'es', 'Lidia']\n",
      "['o', 'l', 'a', 'i', 'n', 'o', 'm', 'b', 'r', 'e', 'e', 's', 'i', 'd', 'i', 'a']\n",
      "['H', 'o', 'l', 'a', 'M', 'i', 'n', 'o', 'm', 'b', 'r', 'e', 'e', 's', 'L', 'i', 'd', 'i', 'a']\n",
      "['Estoy', 'Abu', 'Dhabi']\n",
      "['25', '6']\n"
     ]
    }
   ],
   "source": [
    "# findall - encuentra todos los patrones en una string\n",
    "my_string = 'Hola! Mi nombre es Lidia.'\n",
    "\n",
    "PATTERN = r'\\s+' # espacios (Hay 4 espacios)\n",
    "print(re.findall(PATTERN,my_string))\n",
    "\n",
    "PATTERN = r'\\w+' # palabras\n",
    "print(re.findall(PATTERN,my_string))\n",
    "\n",
    "PATTERN = r'[a-z]' # letras minúsculas (encuentra las letras minúsculas)\n",
    "print(re.findall(PATTERN,my_string))\n",
    "\n",
    "PATTERN = r'\\w' # letras (encuentra letras)\n",
    "print(re.findall(PATTERN,my_string))\n",
    "\n",
    "# el patrón es que muestre todas las palabras que empiezan por mayúsculas\n",
    "capitalized_words = r'[A-Z]\\w+'\n",
    "my_string = 'Estoy volando a Abu Dhabi, qué guay!'\n",
    "\n",
    "print(re.findall(capitalized_words,my_string))\n",
    "\n",
    "# Escribir todos los dígitos de mi cadena de texto\n",
    "my_string = 'Tengo 25 años y el vuelo a Abu Dhabi tarda 6 horas.'\n",
    "digits = r'\\d+'\n",
    "print(re.findall(digits,my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ' Mi nombre es Lidia', ' Tengo 25 años', '']\n",
      "['Estoy', 'volando', 'a', 'Abu', 'Dhabi,', 'qué', 'guay!']\n"
     ]
    }
   ],
   "source": [
    "# split - Nos separa la frase en función de los patrones que le digamos\n",
    "\n",
    "# el patrón que le acabamos de decir es que nos separe por los signos de puntuación .,? ó !\n",
    "my_string = 'Hola. Mi nombre es Lidia. Tengo 25 años.'\n",
    "sentence_endings = r'[.?!]'\n",
    "\n",
    "print(re.split(sentence_endings,my_string))\n",
    "\n",
    "# el patrón es que separe por espacios\n",
    "spaces = r'\\s+'\n",
    "my_string = 'Estoy volando a Abu Dhabi, qué guay!'\n",
    "\n",
    "print(re.split(spaces,my_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Es importante que cuando hagamos el patrón pongamos `r`delante para asegurarnos que nuestros patrones son interpretados de la manera que queremos. \n",
    "Por ejemplo, `\\n` en Python se utiliza para indicar una nueva línea, pero si se usa el prefijo `r` será interpretado como una cadena de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferencia entre *re.search()* y *re.match()*\n",
    "\n",
    "* `match` intentará hacer un 'match' a una cadena de texto desde el principio.\n",
    "* `search` hará una búsqueda a traves de toda la cadena de texto para buscar opciones de 'match'\n",
    "\n",
    "Si se necesita encontrar un patrón pero no necesariamente tiene que estar a principio de palabra o de cadena de texto se recomienda usar `search`.\n",
    "\n",
    "En caso de que tu quieras una composición específica de una cadena de texto completa o al menos un patrón inicial se recomienda usar `match`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utilizando match <_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
      "utilizando search <_sre.SRE_Match object; span=(0, 3), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "# aquí hacen lo mismo \n",
    "print('utilizando match',re.match('abc','abcde'))\n",
    "print('utilizando search',re.search('abc','abcde'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utilizando match None\n",
      "utilizando search <_sre.SRE_Match object; span=(2, 4), match='cd'>\n"
     ]
    }
   ],
   "source": [
    "# aquí match no encuentra nada puesto que la expresión que buscamos se encuentra a mitad de palabra\n",
    "print('utilizando match',re.match('cd','abcde'))\n",
    "print('utilizando search',re.search('cd','abcde'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(27, 34), match='tickets'>\n",
      "27 34\n"
     ]
    }
   ],
   "source": [
    "example =\"I didn't notice I left the tickets in the restaurant. I'm sorry...\"\n",
    "\n",
    "# Utilizamos search\n",
    "match = re.search('tickets',example)\n",
    "print(match)\n",
    "#Dónde empieza y dónde acaba mi match\n",
    "print(match.start(),match.end())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción a la Tokenización\n",
    "\n",
    "* Convertir una cadena de texto o un documento en pequeños 'chunks'.\n",
    "* Es el paso previo a hacer NLP\n",
    "* Se pueden utilizar reglas propias utilizando expresiones regulares\n",
    "* Ejemplos:\n",
    "            - Separar palabras o frases\n",
    "            - Separar signos de puntuación\n",
    "            - Separar los hastags en un tweet\n",
    "            \n",
    "\n",
    "Se utilizará la librería `nltk` natural language toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'do', \"n't\", 'like', 'fish', 'and', 'chips']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# librería de nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# ejemplo tokenización en inglés\n",
    "word_tokenize(\"I don't like fish and chips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenización**\n",
    "\n",
    "- Facilita el mapeo en una parte del discurso\n",
    "- Encuentra 'matchs' con palabras comunes\n",
    "- Eliminamos palabras no deseadas\n",
    "\n",
    "Si nosotros tokenizamos la frase \"I don't like Sam's shoes\", podemos ver claramente una negación con el 'not' y una posesión con 's. Esos indicadores nos pueden ayudar a determinar el significado de un texto simple.\n",
    "\n",
    "Derivados de nltk:\n",
    "* `sent_tokenize`: tokeniza documento en frases \n",
    "* `regexp_tokenize`:  tokeniza documentos o frases según una expresión regular\n",
    "* `TweetTokenizer`: especial para la tokenización de los tweets, permitiendo separar hastags, menciones y los signos de exclamación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZAR DOCUMENTO EN FRASES\n",
      "['Hi Jack!', 'I would like to invite you to my home tonight.']\n",
      "=============================\n",
      "TOKENIZAMOS FRASE 1\n",
      "['Hi', 'Jack', '!']\n",
      "TOKENIZAMOS FRASE 2\n",
      "['I', 'would', 'like', 'to', 'invite', 'you', 'to', 'my', 'home', 'tonight', '.']\n"
     ]
    }
   ],
   "source": [
    "example = 'Hi Jack! I would like to invite you to my home tonight.'\n",
    "\n",
    "# Tokenizamos el documento en frases\n",
    "# comando sent\n",
    "sentences = sent_tokenize(example)\n",
    "print('TOKENIZAR DOCUMENTO EN FRASES')\n",
    "print(sentences)\n",
    "\n",
    "print('=============================')\n",
    "\n",
    "# Tokenizamos las frases\n",
    "# comando word\n",
    "tokenized_sent = word_tokenize(sentences[0])\n",
    "print('TOKENIZAMOS FRASE 1')\n",
    "print(tokenized_sent)\n",
    "\n",
    "\n",
    "tokenized_sent2 = word_tokenize(sentences[1])\n",
    "print('TOKENIZAMOS FRASE 2')\n",
    "print(tokenized_sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenización avanzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Representamos 'OR' con el símbolo | \n",
    "* Podemos representar un grupo de patrones utilizando ()\n",
    "* Puedes definir un carácter explícito utilizando []\n",
    "\n",
    "\n",
    "\n",
    "|   pattern   |matches      | example    |\n",
    "|---------------|----------------|----------------|\n",
    "| [A-Za-z]+    |  upper and lowecase English alphabet  | 'ABCDEfjhk' |\n",
    "| [0-9]    | numbers from 0 to 9  |  9 |             |\n",
    "| [A-Za-z\\-\\.]+|  upper and lowercase English alphabet, - and .          | 'My-website.com'            | \n",
    "|(a-z) | a,- and z|'a-z'\n",
    "|(\\s+l,) |spaces or comma |','|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# patrón\n",
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "re.findall(match_digits_and_words, 'he has 11 cats!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['SOLDIER', '#1', 'Found', 'them', '?', 'In', 'Mercea', '?', 'The', 'coconut', 's', 'tropical', '!']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo para profundizar\n",
    "my_string = \"SOLDIER #1:Found them? In Mercea? The coconut's tropical!\"\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "# una palabra que acabe con una exclamación o interrogación\n",
    "pattern1 = r'\\w+(\\?!)' \n",
    "print(regexp_tokenize(my_string,pattern1))\n",
    "\n",
    "pattern2 = r'(\\w+|#\\d|\\?|!)'\n",
    "print(regexp_tokenize(my_string,pattern2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first tweet\n",
      "['#nlp', '#python']\n",
      "========================================\n",
      "The last tweet\n",
      "['@datacamp', '#nlp', '#python']\n",
      "==================================\n",
      "All Tweets\n",
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "# ejemplos con tweets\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']\n",
    "\n",
    "# Regexp Tokenize\n",
    "# Definimos un patrón para encontrar los hastags seguido de una palabra\n",
    "pattern1 = r'#\\w+'\n",
    "\n",
    "# Utilizamos el patrón en primer tweet\n",
    "print('The first tweet')\n",
    "print(regexp_tokenize(tweets[0],pattern1))\n",
    "\n",
    "print('========================================')\n",
    "\n",
    "# Escribir un patrón que nos muestre las menciones y los hastags\n",
    "pattern2 = r'([#|@]\\w+)'\n",
    "\n",
    "# lo utilizamos en el ultimo tweet de mi lista\n",
    "print('The last tweet')\n",
    "print(regexp_tokenize(tweets[-1],pattern2))\n",
    "\n",
    "print('==================================')\n",
    "# Tweet Tokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print('All Tweets')\n",
    "print(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo avanzado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scene_one =\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\"\n",
    "\n",
    "# echar un vistazo al texto\n",
    "scene_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCENE 1: [wind] [clop clop clop] ', 'KING ARTHUR: Whoa there!  [clop clop clop] ', 'SOLDIER #1: Halt!  Who goes there?', 'ARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!']\n"
     ]
    }
   ],
   "source": [
    "# Separamos el texto en frases, nos damos cuenta de que \n",
    "# tiene \\n como separador por tanto:\n",
    "lines = scene_one.split('\\n')\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCENE 1: [wind] [clop clop clop] ', ' Whoa there!  [clop clop clop] ', ' Halt!  Who goes there?', ' It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!']\n"
     ]
    }
   ],
   "source": [
    "# Para analizar el texto nos interesa eliminar quién habla\n",
    "# Reemplazamos los nombres por vacío\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines = [re.sub(pattern,'',l) for l in lines]\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SCENE', '1', 'wind', 'clop', 'clop', 'clop'], ['Whoa', 'there', 'clop', 'clop', 'clop'], ['Halt', 'Who', 'goes', 'there'], ['It', 'is', 'I', 'Arthur', 'son', 'of', 'Uther', 'Pendragon', 'from', 'the', 'castle', 'of', 'Camelot', 'King', 'of', 'the', 'Britons', 'defeator', 'of', 'the', 'Saxons', 'sovereign', 'of', 'all', 'England']]\n"
     ]
    }
   ],
   "source": [
    "# Ahora que tenemos el texto sin speakers y en frases, tokenizamos cada frase\n",
    "# Ojo! se puede hacer con word_tokenizer también!\n",
    "pattern = '\\w+'\n",
    "\n",
    "tokenized_lines = []\n",
    "for s in lines:\n",
    "    x = regexp_tokenize(s,pattern)\n",
    "    tokenized_lines.append(x)\n",
    "\n",
    "print(tokenized_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 5, 4, 25]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b3c55235f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hacemos una frecuencia de la LONGITUD de la frase\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "print(line_num_words)\n",
    "\n",
    "# Plot de un histograma con la frecuencia de la longitud\n",
    "plt.hist(line_num_words,bins=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
